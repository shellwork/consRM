import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_recall_fscore_support, 
                           roc_auc_score, average_precision_score, 
                           classification_report, confusion_matrix)
from sklearn.pipeline import Pipeline
from collections import Counter
import itertools
import warnings
warnings.filterwarnings('ignore')


def extract_kmer_features(sequences, k=4):
    """æå–k-merç‰¹å¾"""
    print(f"æå– {k}-mer ç‰¹å¾...")
    
    # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„k-mer
    nucleotides = ['A', 'T', 'G', 'C']
    all_kmers = [''.join(p) for p in itertools.product(nucleotides, repeat=k)]
    
    features = []
    for seq in sequences:
        # è®¡ç®—æ¯ä¸ªåºåˆ—çš„k-meré¢‘ç‡
        kmer_counts = Counter([seq[i:i+k] for i in range(len(seq)-k+1) if len(seq[i:i+k]) == k])
        # è½¬æ¢ä¸ºç‰¹å¾å‘é‡
        feature_vector = [kmer_counts.get(kmer, 0) for kmer in all_kmers]
        features.append(feature_vector)
    
    return np.array(features), all_kmers

def extract_basic_sequence_features(sequences):
    """æå–åŸºç¡€åºåˆ—ç‰¹å¾"""
    print("æå–åŸºç¡€åºåˆ—ç‰¹å¾...")
    
    features = []
    for seq in sequences:
        seq_len = len(seq)
        if seq_len == 0:
            features.append([0] * 8)
            continue
            
        # è®¡ç®—ç¢±åŸºç»„æˆ
        a_count = seq.count('A') / seq_len
        t_count = seq.count('T') / seq_len
        g_count = seq.count('G') / seq_len
        c_count = seq.count('C') / seq_len
        
        # GCå«é‡
        gc_content = g_count + c_count
        
        # ATå«é‡
        at_content = a_count + t_count
        
        # åºåˆ—é•¿åº¦
        seq_length = seq_len
        
        # åºåˆ—å¤æ‚åº¦ï¼ˆç®€å•ä¼°è®¡ï¼šä¸åŒç¢±åŸºçš„æ•°é‡ï¼‰
        unique_bases = len(set(seq))
        
        features.append([a_count, t_count, g_count, c_count, gc_content, at_content, seq_length, unique_bases])
    
    return np.array(features)

def check_data_quality(sequences, labels):
    """æ£€æŸ¥æ•°æ®è´¨é‡"""
    print("\n" + "="*50)
    print("æ•°æ®è´¨é‡æ£€æŸ¥")
    print("="*50)
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"æ€»æ ·æœ¬æ•°: {len(sequences)}")
    print(f"æ€»æ ‡ç­¾æ•°: {len(labels)}")
    
    # æ£€æŸ¥åºåˆ—é•¿åº¦
    seq_lengths = [len(seq) for seq in sequences]
    print(f"åºåˆ—é•¿åº¦ç»Ÿè®¡:")
    print(f"  æœ€å°é•¿åº¦: {min(seq_lengths)}")
    print(f"  æœ€å¤§é•¿åº¦: {max(seq_lengths)}")
    print(f"  å¹³å‡é•¿åº¦: {np.mean(seq_lengths):.1f}")
    print(f"  ä¸­ä½æ•°é•¿åº¦: {np.median(seq_lengths):.1f}")
    
    # æ£€æŸ¥åºåˆ—å”¯ä¸€æ€§
    unique_sequences = len(set(sequences))
    duplicate_count = len(sequences) - unique_sequences
    print(f"åºåˆ—å”¯ä¸€æ€§:")
    print(f"  å”¯ä¸€åºåˆ—æ•°: {unique_sequences}")
    print(f"  é‡å¤åºåˆ—æ•°: {duplicate_count}")
    print(f"  é‡å¤æ¯”ä¾‹: {duplicate_count/len(sequences)*100:.2f}%")
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    unique_labels, label_counts = np.unique(labels, return_counts=True)
    print(f"æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in zip(unique_labels, label_counts):
        print(f"  æ ‡ç­¾ {label}: {count} ({count/len(labels)*100:.2f}%)")
    
    # ç±»åˆ«å¹³è¡¡
    balance_ratio = label_counts.min() / label_counts.max()
    print(f"ç±»åˆ«å¹³è¡¡æ¯”: {balance_ratio:.3f}")
    
    # æ£€æŸ¥åºåˆ—ç›¸ä¼¼æ€§ï¼ˆæŠ½æ ·æ£€æŸ¥ï¼‰
    print("æ£€æŸ¥åºåˆ—ç›¸ä¼¼æ€§ï¼ˆæŠ½æ ·å‰100ä¸ªåºåˆ—ï¼‰...")
    similarity_threshold = 0.9
    high_similarity_pairs = 0
    sample_size = min(100, len(sequences))
    
    from difflib import SequenceMatcher
    for i in range(sample_size):
        for j in range(i+1, sample_size):
            similarity = SequenceMatcher(None, sequences[i], sequences[j]).ratio()
            if similarity > similarity_threshold:
                high_similarity_pairs += 1
                if high_similarity_pairs <= 5:  # åªæ‰“å°å‰5å¯¹
                    print(f"  é«˜ç›¸ä¼¼åºåˆ—å¯¹ {i}-{j}: ç›¸ä¼¼åº¦ {similarity:.3f}")
    
    print(f"é«˜ç›¸ä¼¼åº¦åºåˆ—å¯¹æ•° (>{similarity_threshold}): {high_similarity_pairs}")
    
    # è­¦å‘Š
    if duplicate_count > 0:
        print("âš ï¸  è­¦å‘Š: å‘ç°é‡å¤åºåˆ—!")
    if balance_ratio < 0.1:
        print("âš ï¸  è­¦å‘Š: ç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡!")
    if high_similarity_pairs > 10:
        print("âš ï¸  è­¦å‘Š: å‘ç°å¤§é‡é«˜ç›¸ä¼¼åº¦åºåˆ—!")
    
    print("="*50)
    return {
        'duplicate_count': duplicate_count,
        'balance_ratio': balance_ratio,
        'high_similarity_pairs': high_similarity_pairs
    }

def load_and_preprocess_data(data_path):
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ï¼ˆä¸åŸä»£ç ä¿æŒä¸€è‡´ï¼‰"""
    print(f"åŠ è½½æ•°æ®: {data_path}")
    df = pd.read_csv(data_path)
    
    # æå–åºåˆ—å’Œæ ‡ç­¾ï¼ˆä¸åŸä»£ç ä¸€è‡´ï¼‰
    sequences = df.iloc[:, 0].values  # ç¬¬ä¸€åˆ—æ˜¯åºåˆ—
    labels = df.iloc[:, -1].values    # æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾
    
    # æ•°æ®æ¸…æ´—
    valid_indices = []
    for i, seq in enumerate(sequences):
        # æ£€æŸ¥åºåˆ—æ˜¯å¦æœ‰æ•ˆ
        if isinstance(seq, str) and len(seq) > 0:
            # æ£€æŸ¥æ˜¯å¦åªåŒ…å«DNAç¢±åŸº
            if all(base in 'ATGC' for base in seq.upper()):
                valid_indices.append(i)
    
    sequences = [str(sequences[i]).upper() for i in valid_indices]
    labels = labels[valid_indices]
    
    print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(sequences)}")
    
    return sequences, labels

def train_baseline_models(X_train, X_val, y_train, y_val, feature_names="features"):
    """è®­ç»ƒå¤šä¸ªåŸºçº¿æ¨¡å‹"""
    print(f"\nè®­ç»ƒåŸºçº¿æ¨¡å‹ ({feature_names})...")
    
    models = {
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'Logistic Regression (Balanced)': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),
        'Logistic Regression (L1)': LogisticRegression(random_state=42, max_iter=1000, penalty='l1', solver='liblinear'),
        'Logistic Regression (L2 Strong)': LogisticRegression(random_state=42, max_iter=1000, C=0.1),
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_val)
        y_prob = model.predict_proba(X_val)[:, 1]  # æ­£ç±»æ¦‚ç‡
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_val, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')
        auc = roc_auc_score(y_val, y_prob)
        auprc = average_precision_score(y_val, y_prob)
        
        results[name] = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'auprc': auprc
        }
        
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"  å¬å›ç‡: {recall:.4f}")
        print(f"  F1åˆ†æ•°: {f1:.4f}")
        print(f"  AUC: {auc:.4f}")
        print(f"  AUPRC: {auprc:.4f}")
        
        # å¦‚æœAUCæˆ–AUPRCå¼‚å¸¸é«˜ï¼Œç»™å‡ºè­¦å‘Š
        if auc > 0.95 or auprc > 0.95:
            print(f"  âš ï¸  è­¦å‘Š: {name} çš„ AUC({auc:.4f}) æˆ– AUPRC({auprc:.4f}) å¼‚å¸¸é«˜!")
    
    return results

# åœ¨æ•°æ®åˆ’åˆ†åæ·»åŠ ç›¸ä¼¼æ€§æ£€æŸ¥
def check_set_similarity(train_seq, val_seq, threshold=0.95):
    """æ£€æŸ¥è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¹‹é—´çš„åºåˆ—ç›¸ä¼¼æ€§"""
    from difflib import SequenceMatcher
    similar_count = 0
    
    print("\næ£€æŸ¥è®­ç»ƒé›†-éªŒè¯é›†åºåˆ—ç›¸ä¼¼æ€§...")
    for i, train_seq_i in enumerate(train_seq):
        for j, val_seq_j in enumerate(val_seq):
            if SequenceMatcher(None, train_seq_i, val_seq_j).ratio() > threshold:
                similar_count += 1
                if similar_count <= 5:  # æ‰“å°å‰5ä¸ªä¾‹å­
                    print(f"  é«˜ç›¸ä¼¼åºåˆ—å¯¹: è®­ç»ƒ[{i}] vs éªŒè¯[{j}]")
    
    print(f"ç›¸ä¼¼åº¦>{threshold}çš„åºåˆ—å¯¹æ•°: {similar_count}")
    print(f"å æ€»éªŒè¯é›†æ¯”ä¾‹: {similar_count/len(val_seq):.2%}")
    return similar_count



def main():
    # é…ç½®
    data_path = 'data/c1.csv'  # è¯·æ›¿æ¢ä¸ºä½ çš„æ•°æ®è·¯å¾„
    test_size = 0.2
    random_state = 42
    
    print("DNAä¿å®ˆæ€§é¢„æµ‹ - åŸºçº¿æ¨¡å‹æµ‹è¯•")
    print("="*60)
    
    # åŠ è½½æ•°æ®
    sequences, labels = load_and_preprocess_data(data_path)
    
    # æ•°æ®è´¨é‡æ£€æŸ¥
    quality_info = check_data_quality(sequences, labels)
    
    # æ•°æ®åˆ’åˆ†ï¼ˆä¸åŸä»£ç ä¸€è‡´ï¼‰
    train_seq, val_seq, train_labels, val_labels = train_test_split(
        sequences, labels,
        test_size=test_size,
        random_state=random_state,
        stratify=labels
    )
    
    print(f"\næ•°æ®åˆ’åˆ†:")
    print(f"è®­ç»ƒé›†: {len(train_seq)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_seq)} æ ·æœ¬")
    print(f"è®­ç»ƒé›†æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(train_labels):.3f}")
    print(f"éªŒè¯é›†æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(val_labels):.3f}")
    
    # # åœ¨æ•°æ®åˆ’åˆ†åè°ƒç”¨
    # similar_count = check_set_similarity(train_seq, val_seq)
    
    # 1. åŸºç¡€åºåˆ—ç‰¹å¾
    print("\n" + "="*60)
    print("æµ‹è¯•1: åŸºç¡€åºåˆ—ç‰¹å¾")
    print("="*60)
    
    basic_train_features = extract_basic_sequence_features(train_seq)
    basic_val_features = extract_basic_sequence_features(val_seq)
    
    # æ ‡å‡†åŒ–
    scaler_basic = StandardScaler()
    basic_train_features_scaled = scaler_basic.fit_transform(basic_train_features)
    basic_val_features_scaled = scaler_basic.transform(basic_val_features)
    
    basic_results = train_baseline_models(
        basic_train_features_scaled, basic_val_features_scaled,
        train_labels, val_labels, "åŸºç¡€åºåˆ—ç‰¹å¾"
    )
    
    # 2. 3-merç‰¹å¾
    print("\n" + "="*60)
    print("æµ‹è¯•2: 3-merç‰¹å¾")
    print("="*60)
    
    kmer3_train_features, kmer3_names = extract_kmer_features(train_seq, k=3)
    kmer3_val_features, _ = extract_kmer_features(val_seq, k=3)
    
    # æ ‡å‡†åŒ–
    scaler_kmer3 = StandardScaler()
    kmer3_train_features_scaled = scaler_kmer3.fit_transform(kmer3_train_features)
    kmer3_val_features_scaled = scaler_kmer3.transform(kmer3_val_features)
    
    kmer3_results = train_baseline_models(
        kmer3_train_features_scaled, kmer3_val_features_scaled,
        train_labels, val_labels, "3-merç‰¹å¾"
    )
    
    # 3. 4-merç‰¹å¾
    print("\n" + "="*60)
    print("æµ‹è¯•3: 4-merç‰¹å¾")
    print("="*60)
    
    kmer4_train_features, kmer4_names = extract_kmer_features(train_seq, k=4)
    kmer4_val_features, _ = extract_kmer_features(val_seq, k=4)
    
    # æ ‡å‡†åŒ–
    scaler_kmer4 = StandardScaler()
    kmer4_train_features_scaled = scaler_kmer4.fit_transform(kmer4_train_features)
    kmer4_val_features_scaled = scaler_kmer4.transform(kmer4_val_features)
    
    kmer4_results = train_baseline_models(
        kmer4_train_features_scaled, kmer4_val_features_scaled,
        train_labels, val_labels, "4-merç‰¹å¾"
    )
    
    # 4. ç»„åˆç‰¹å¾
    print("\n" + "="*60)
    print("æµ‹è¯•4: ç»„åˆç‰¹å¾ (åŸºç¡€ + 3-mer)")
    print("="*60)
    
    combined_train_features = np.hstack([basic_train_features_scaled, kmer3_train_features_scaled])
    combined_val_features = np.hstack([basic_val_features_scaled, kmer3_val_features_scaled])
    
    combined_results = train_baseline_models(
        combined_train_features, combined_val_features,
        train_labels, val_labels, "ç»„åˆç‰¹å¾"
    )
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*80)
    print("åŸºçº¿æ¨¡å‹ç»“æœæ±‡æ€»")
    print("="*80)
    
    all_results = {
        "åŸºç¡€ç‰¹å¾": basic_results,
        "3-merç‰¹å¾": kmer3_results,
        "4-merç‰¹å¾": kmer4_results,
        "ç»„åˆç‰¹å¾": combined_results
    }
    
    # æ‰¾å‡ºæœ€é«˜çš„AUCå’ŒAUPRC
    max_auc = 0
    max_auprc = 0
    
    for feature_type, results in all_results.items():
        print(f"\n{feature_type}:")
        for model_name, metrics in results.items():
            auc = metrics['auc']
            auprc = metrics['auprc']
            print(f"  {model_name}: AUC={auc:.4f}, AUPRC={auprc:.4f}")
            max_auc = max(max_auc, auc)
            max_auprc = max(max_auprc, auprc)
    
    # æœ€ç»ˆåˆ¤æ–­
    print("\n" + "="*80)
    print("æ•°æ®å¼‚å¸¸æ£€æµ‹ç»“æœ")
    print("="*80)
    
    print(f"æœ€é«˜ AUC: {max_auc:.4f}")
    print(f"æœ€é«˜ AUPRC: {max_auprc:.4f}")
    
    # åˆ¤æ–­æ ‡å‡†
    suspicious_auc = max_auc > 0.90
    suspicious_auprc = max_auprc > 0.90
    
    if suspicious_auc or suspicious_auprc:
        print("\nğŸš¨ æ•°æ®å¼‚å¸¸è­¦å‘Š!")
        print("ç®€å•çš„çº¿æ€§æ¨¡å‹å°±èƒ½è¾¾åˆ°å¾ˆé«˜çš„æ€§èƒ½ï¼Œè¿™é€šå¸¸è¡¨æ˜:")
        print("1. æ•°æ®å­˜åœ¨æ³„éœ²ï¼ˆé‡å¤åºåˆ—ã€ç›¸ä¼¼åºåˆ—ç­‰ï¼‰")
        print("2. ä»»åŠ¡å¯èƒ½æ¯”é¢„æœŸç®€å•")
        print("3. æ ‡ç­¾è´¨é‡å¯èƒ½æœ‰é—®é¢˜")
        print("\nå»ºè®®:")
        print("- æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„åºåˆ—")
        print("- é‡æ–°å®¡è§†æ•°æ®æ”¶é›†å’Œæ ‡æ³¨è¿‡ç¨‹")
        print("- ä½¿ç”¨æ›´ä¸¥æ ¼çš„äº¤å‰éªŒè¯æ–¹æ³•")
        print("- è€ƒè™‘ä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•é›†")
    else:
        print("\nâœ… åŸºçº¿ç»“æœæ­£å¸¸")
        print("ç®€å•æ¨¡å‹çš„æ€§èƒ½åœ¨åˆç†èŒƒå›´å†…ï¼ŒDNABERT2çš„é«˜æ€§èƒ½å¯èƒ½æ˜¯çœŸå®çš„ã€‚")
    
    # æ•°æ®è´¨é‡æ€»ç»“
    print(f"\næ•°æ®è´¨é‡é—®é¢˜æ€»ç»“:")
    print(f"- é‡å¤åºåˆ—æ•°: {quality_info['duplicate_count']}")
    print(f"- ç±»åˆ«å¹³è¡¡æ¯”: {quality_info['balance_ratio']:.3f}")
    print(f"- é«˜ç›¸ä¼¼åºåˆ—å¯¹: {quality_info['high_similarity_pairs']}")

if __name__ == "__main__":
    main()