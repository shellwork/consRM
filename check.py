# æ•°æ®æ³„éœ²æ£€æŸ¥ä»£ç 
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

def check_data_leakage(data_path):
    """æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦å­˜åœ¨é‡å¤æ ·æœ¬æˆ–æ³„éœ²"""
    
    print("=== æ•°æ®æ³„éœ²æ£€æŸ¥ ===")
    
    # 1. åŠ è½½åŸå§‹æ•°æ®
    df = pd.read_csv(data_path)
    sequences = df.iloc[:, 0].values
    labels = df.iloc[:, -1].values
    
    print(f"æ€»æ ·æœ¬æ•°: {len(sequences)}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)}")
    
    # 2. æ£€æŸ¥åºåˆ—é‡å¤
    unique_sequences = set(sequences)
    print(f"å”¯ä¸€åºåˆ—æ•°: {len(unique_sequences)}")
    print(f"é‡å¤åºåˆ—æ•°: {len(sequences) - len(unique_sequences)}")
    
    if len(unique_sequences) != len(sequences):
        print("âš ï¸  å‘ç°é‡å¤åºåˆ—ï¼")
        
        # æ‰¾å‡ºé‡å¤çš„åºåˆ—
        seq_counts = {}
        for seq in sequences:
            seq_counts[seq] = seq_counts.get(seq, 0) + 1
        
        duplicates = {seq: count for seq, count in seq_counts.items() if count > 1}
        print(f"é‡å¤åºåˆ—ç¤ºä¾‹: {list(duplicates.items())[:5]}")
    
    # 3. æ£€æŸ¥åºåˆ—é•¿åº¦åˆ†å¸ƒ
    seq_lengths = [len(seq) for seq in sequences]
    print(f"åºåˆ—é•¿åº¦ç»Ÿè®¡:")
    print(f"  æœ€å°é•¿åº¦: {min(seq_lengths)}")
    print(f"  æœ€å¤§é•¿åº¦: {max(seq_lengths)}")
    print(f"  å¹³å‡é•¿åº¦: {np.mean(seq_lengths):.2f}")
    print(f"  é•¿åº¦æ ‡å‡†å·®: {np.std(seq_lengths):.2f}")
    
    # 4. æ¨¡æ‹Ÿä½ çš„æ•°æ®åˆ’åˆ†
    train_seq, val_seq, train_labels, val_labels = train_test_split(
        sequences, labels,
        test_size=0.2,
        random_state=42,  # ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­
        stratify=labels
    )
    
    # 5. æ£€æŸ¥è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¹‹é—´çš„é‡å 
    train_set = set(train_seq)
    val_set = set(val_seq)
    overlap = train_set.intersection(val_set)
    
    print(f"\n=== è®­ç»ƒ/éªŒè¯é›†é‡å æ£€æŸ¥ ===")
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_seq)}")
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_seq)}")
    print(f"é‡å æ ·æœ¬æ•°: {len(overlap)}")
    
    if len(overlap) > 0:
        print("ğŸš¨ å‘ç°è®­ç»ƒé›†å’ŒéªŒè¯é›†é‡å ï¼")
        print(f"é‡å æ ·æœ¬ç¤ºä¾‹: {list(overlap)[:3]}")
    else:
        print("âœ… è®­ç»ƒé›†å’ŒéªŒè¯é›†æ— é‡å ")
    
    # 6. æ£€æŸ¥ç›¸ä¼¼åºåˆ—
    print(f"\n=== åºåˆ—ç›¸ä¼¼æ€§æ£€æŸ¥ ===")
    def hamming_distance(s1, s2):
        if len(s1) != len(s2):
            return float('inf')
        return sum(c1 != c2 for c1, c2 in zip(s1, s2))
    
    # éšæœºé€‰æ‹©ä¸€äº›éªŒè¯é›†åºåˆ—ï¼Œæ£€æŸ¥ä¸è®­ç»ƒé›†çš„æœ€å°è·ç¦»
    sample_val_seqs = np.random.choice(val_seq, min(50, len(val_seq)), replace=False)
    sample_train_seqs = np.random.choice(train_seq, min(100, len(train_seq)), replace=False)
    
    min_distances = []
    for val_seq_sample in sample_val_seqs:
        distances = [hamming_distance(val_seq_sample, train_seq_sample) 
                    for train_seq_sample in sample_train_seqs]
        min_distances.append(min(distances))
    
    print(f"éªŒè¯é›†åºåˆ—ä¸è®­ç»ƒé›†æœ€å°è·ç¦»ç»Ÿè®¡:")
    print(f"  å¹³å‡æœ€å°è·ç¦»: {np.mean(min_distances):.2f}")
    print(f"  æœ€å°è·ç¦»çš„æœ€å°å€¼: {min(min_distances)}")
    print(f"  è·ç¦»ä¸º0çš„æ•°é‡: {sum(1 for d in min_distances if d == 0)}")
    
    return {
        'total_samples': len(sequences),
        'unique_sequences': len(unique_sequences),
        'duplicates': len(sequences) - len(unique_sequences),
        'train_val_overlap': len(overlap),
        'min_distance_stats': {
            'mean': np.mean(min_distances),
            'min': min(min_distances),
            'zero_distance_count': sum(1 for d in min_distances if d == 0)
        }
    }

# ä½¿ç”¨æ–¹æ³•
if __name__ == "__main__":
    # æ›¿æ¢ä¸ºä½ çš„æ•°æ®æ–‡ä»¶è·¯å¾„
    results = check_data_leakage('data/c1.csv')
    print(f"\n=== æ£€æŸ¥ç»“æœæ±‡æ€» ===")
    print(results)